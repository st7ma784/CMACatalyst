# ğŸ—ï¸ Separated LLM & Vision Architecture (Phase 1)

**Objective**: Separate vision/OCR models from language models to enable independent scaling and optimization.

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FRONTEND (3000)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RAG Service   â”‚          â”‚  Notes Service    â”‚
    â”‚    (8102)      â”‚          â”‚     (8100)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   OLLAMA (LLM)             â”‚
            â”‚   Port: 11434              â”‚
            â”‚   Models:                  â”‚
            â”‚   - llama3.2:latest        â”‚
            â”‚   - llama2:7b              â”‚
            â”‚   Use: RAG, Chat, NER      â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Doc Processor (8101)               â”‚
    â”‚  OCR Service (8104)                 â”‚
    â”‚  Client RAG (8105)                  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  OLLAMA VISION (VLM/OCR)      â”‚
         â”‚  Port: 11435                  â”‚
         â”‚  Models:                      â”‚
         â”‚  - llava:7b (Vision)          â”‚
         â”‚  - ollama-ocr (Future)        â”‚
         â”‚  Use: Document parsing, OCR   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Graph Database Layer:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Neo4j      â”‚  PostgreSQL      â”‚  ChromaDB    â”‚
â”‚  (7687)     â”‚  (5432)          â”‚  (8005)      â”‚
â”‚  Graph DB   â”‚  Relational DB   â”‚  Vector DB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Phase 1: Separation (Current)

### Benefits
âœ… Independent scaling (vision separate from LLM)
âœ… Ollama vision can handle multiple parallel requests
âœ… LLM Ollama optimized for text-only workloads
âœ… Clear resource allocation
âœ… Foundation for Phase 2 (vLLM)

### Services Configuration

**Ollama (LLM)** - Port 11434
- Models: llama3.2:latest, llama2:7b
- Services: RAG, Notes, NER Graph
- CPU/Memory optimized for text generation

**Ollama Vision** - Port 11435
- Models: llava:7b (and future: ollama-ocr)
- Services: OCR Service, Doc Processor, Client RAG
- Optimized for vision inference

## Phase 2: vLLM Optimization (Future)

Replace Ollama LLM with vLLM for:
- Faster inference (KV cache optimization)
- Better batching
- Token streaming
- Lower latency for chatbot

Vision stays with Ollama or upgrades to specialized VLM server.

## Implementation Steps

1. **Create Vision Ollama Service**
   - New `ollama-entrypoint-vision.sh` for vision models
   - Port 11435
   - Auto-pull llava:7b

2. **Update Main Ollama**
   - Remove vision models from main Ollama
   - Keep llama3.2, llama2

3. **Update Services**
   - Doc Processor: `VISION_OLLAMA_URL=http://ollama-vision:11435`
   - OCR Service: `VISION_OLLAMA_URL=http://ollama-vision:11435`
   - Client RAG: Option to use vision models

4. **Update docker-compose-simple.yml**
   - Two Ollama services with different ports
   - Health checks for each
   - Proper dependencies

## Environment Variables

### LLM Ollama (11434)
```
OLLAMA_URL=http://ollama:11434
OLLAMA_MODEL=llama3.2:latest
```

### Vision Ollama (11435)
```
VISION_OLLAMA_URL=http://ollama-vision:11435
VISION_MODEL=llava:7b
```

## Next Steps After Separation

- Profile resource usage
- Monitor inference times
- Prepare vLLM containers
- Benchmark vLLM vs Ollama for language tasks
- Consider ollama-ocr or similar for vision optimization
