version: '3.8'

# Shared Ollama LLM Service
# This can be used by multiple applications (RAG Demo, OCR Demo, etc.)
# Start this first: docker-compose -f docker-compose.ollama.yml up -d

services:
  ollama:
    image: ollama/ollama:latest
    container_name: shared-ollama-service
    ports:
      - "11434:11434"
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidia-uvm:/dev/nvidia-uvm
      - /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools
      - /dev/nvidiactl:/dev/nvidiactl
    volumes:
      - ollama_models:/root/.ollama
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    environment:
      - OLLAMA_HOST=0.0.0.0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_DEBUG=1
      - LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu
    networks:
      - shared-llm-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Model initialization container
  # Pulls required models on first run
  ollama-init:
    image: curlimages/curl:latest
    container_name: ollama-model-init
    depends_on:
      - ollama
    networks:
      - shared-llm-network
    entrypoint: /bin/sh
    command: >
      -c "
      echo 'Waiting for Ollama to be ready...';
      until curl -sf http://ollama:11434/api/tags > /dev/null; do
        sleep 2;
      done;
      echo 'Ollama is ready!';
      echo 'Models will be pulled on first use by applications';
      echo 'To pre-pull models, run:';
      echo '  docker exec shared-ollama-service ollama pull llama3.2';
      echo '  docker exec shared-ollama-service ollama pull nomic-embed-text';
      "
    restart: "no"

volumes:
  ollama_models:
    name: shared_ollama_models

networks:
  shared-llm-network:
    name: shared-llm-network
    driver: bridge
