# Phase 2: vLLM Implementation for Language Models
# Replaces Ollama LLM service with vLLM for 2-3x faster inference
# Dual-service setup: vLLM for LLM (faster), Vision Ollama for vision/OCR (unchanged)
# Performance: 2-3x faster inference, better batching, streaming support

version: '3.8'

services:
  # ============================================
  # LANGUAGE MODEL SERVICE - vLLM (Phase 2)
  # ============================================
  vllm:
    image: vllm/vllm-openai:v0.3.0
    container_name: rma-vllm
    ports:
      - "8000:8000"
    environment:
      MODEL_NAME: "meta-llama/Llama-2-7b-hf"
      GPU_MEMORY_UTILIZATION: "0.9"
      MAX_MODEL_LEN: "4096"
      TENSOR_PARALLEL_SIZE: "1"
      HUGGINGFACE_HUB_CACHE: "/app/.cache/huggingface/hub"
      CUDA_VISIBLE_DEVICES: "0"  # Use first P100 GPU
    volumes:
      - vllm-cache:/app/.cache
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    networks:
      - rma-network
    restart: unless-stopped
    labels:
      - "com.rma.service=llm"
      - "com.rma.version=vllm-phase2"

  # ============================================
  # vLLM ADAPTER SERVICE
  # ============================================
  vllm-adapter:
    build:
      context: ./services/vllm-service
      dockerfile: Dockerfile.adapter
    container_name: rma-vllm-adapter
    ports:
      - "11434:8000"
    environment:
      VLLM_URL: "http://vllm:8000"
    depends_on:
      vllm:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - rma-network
    restart: unless-stopped
    labels:
      - "com.rma.service=llm-adapter"

  # ============================================
  # VISION MODEL SERVICE - Ollama (Phase 1)
  # ============================================
  ollama-vision:
    image: ollama/ollama:latest
    container_name: rma-ollama-vision
    ports:
      - "11435:11434"
    volumes:
      - ollama-vision-data:/root/.ollama
      - ./ollama-entrypoint-vision.sh:/usr/local/bin/entrypoint.sh
    entrypoint: /usr/local/bin/entrypoint.sh
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      CUDA_VISIBLE_DEVICES: "1"  # Use second P100 GPU
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    networks:
      - rma-network
    restart: unless-stopped

  # ============================================
  # RAG SERVICE
  # ============================================
  rag-service:
    build:
      context: ./services/rag-service
      dockerfile: Dockerfile
    container_name: rma-rag-service
    ports:
      - "8102:8000"
    environment:
      OLLAMA_URL: "http://vllm-adapter:8000"
      OLLAMA_MODEL: "llama2:7b"
      CHROMA_HOST: "chromadb"
      CHROMA_PORT: "8000"
      ENVIRONMENT: "production"
    depends_on:
      vllm-adapter:
        condition: service_healthy
      chromadb:
        condition: service_healthy
    networks:
      - rma-network
    restart: unless-stopped

  # ============================================
  # NOTES SERVICE
  # ============================================
  notes-service:
    build:
      context: ./services/notes-service
      dockerfile: Dockerfile
    container_name: rma-notes-service
    ports:
      - "8100:8000"
    environment:
      OLLAMA_URL: "http://vllm-adapter:8000"
      OLLAMA_MODEL: "llama2:7b"
      ENVIRONMENT: "production"
    depends_on:
      vllm-adapter:
        condition: service_healthy
    networks:
      - rma-network
    restart: unless-stopped

  # ============================================
  # NER SERVICE
  # ============================================
  ner-service:
    build:
      context: ./services/ner-graph-service
      dockerfile: Dockerfile
    container_name: rma-ner-service
    ports:
      - "8108:8000"
    environment:
      OLLAMA_URL: "http://vllm-adapter:8000"
      OLLAMA_MODEL: "llama2:7b"
      NEO4J_HOST: "neo4j"
      NEO4J_PORT: "7687"
      NEO4J_USER: "neo4j"
      NEO4J_PASSWORD: "neo4jpassword"
    depends_on:
      vllm-adapter:
        condition: service_healthy
      neo4j:
        condition: service_healthy
    networks:
      - rma-network
    restart: unless-stopped

  # ============================================
  # VISION/OCR SERVICES
  # ============================================
  doc-processor:
    build:
      context: ./services/doc-processor
      dockerfile: Dockerfile
    container_name: rma-doc-processor
    ports:
      - "8101:8000"
    environment:
      VISION_OLLAMA_URL: "http://ollama-vision:11434"
      VISION_MODEL: "llava:7b"
      ENVIRONMENT: "production"
    depends_on:
      ollama-vision:
        condition: service_healthy
    networks:
      - rma-network
    restart: unless-stopped

  ocr-service:
    build:
      context: ./services/ocr-service
      dockerfile: Dockerfile
    container_name: rma-ocr-service
    ports:
      - "8104:8000"
    environment:
      VISION_OLLAMA_URL: "http://ollama-vision:11434"
      VISION_MODEL: "llava:7b"
      ENVIRONMENT: "production"
    depends_on:
      ollama-vision:
        condition: service_healthy
    networks:
      - rma-network
    restart: unless-stopped

  client-rag-service:
    build:
      context: ./services/client-rag-service
      dockerfile: Dockerfile
    container_name: rma-client-rag-service
    ports:
      - "8105:8000"
    environment:
      VISION_OLLAMA_URL: "http://ollama-vision:11434"
      VISION_MODEL: "llava:7b"
      CHROMA_HOST: "chromadb"
      CHROMA_PORT: "8000"
      ENVIRONMENT: "production"
    depends_on:
      ollama-vision:
        condition: service_healthy
      chromadb:
        condition: service_healthy
    networks:
      - rma-network
    restart: unless-stopped

  # ============================================
  # UPLOAD SERVICE
  # ============================================
  upload-service:
    build:
      context: ./services/upload-service
      dockerfile: Dockerfile
    container_name: rma-upload-service
    ports:
      - "8103:8000"
    environment:
      ENVIRONMENT: "production"
    networks:
      - rma-network
    restart: unless-stopped

  # ============================================
  # DATABASES
  # ============================================
  neo4j:
    image: neo4j:5.15-enterprise
    container_name: rma-neo4j
    ports:
      - "7687:7687"
      - "7474:7474"
    environment:
      NEO4J_AUTH: "neo4j/neo4jpassword"
      NEO4J_dbms_memory_heap_max__size: "1g"
      NEO4J_PLUGINS: '["apoc"]'
    volumes:
      - neo4j-data:/var/lib/neo4j/data
    healthcheck:
      test: ["CMD-SHELL", "cypher-shell -u neo4j -p neo4jpassword 'RETURN 1'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - rma-network
    restart: unless-stopped

  postgres:
    image: postgres:16-alpine
    container_name: rma-postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: "rma_user"
      POSTGRES_PASSWORD: "rma_password"
      POSTGRES_DB: "rma_db"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/01-init.sql
      - ./database/schema.sql:/docker-entrypoint-initdb.d/02-schema.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U rma_user -d rma_db"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - rma-network
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: rma-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - rma-network
    restart: unless-stopped

  chromadb:
    image: ghcr.io/chroma-core/chroma:0.4.24
    container_name: rma-chromadb
    ports:
      - "8005:8000"
    environment:
      CHROMA_DB_IMPL: "duckdb+parquet"
      PERSIST_DIRECTORY: "/chroma/data"
    volumes:
      - chromadb-data:/chroma/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/heartbeat"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - rma-network
    restart: unless-stopped

  # ============================================
  # FRONTEND
  # ============================================
  frontend:
    build:
      context: ./
      dockerfile: Dockerfile.frontend
    container_name: rma-frontend
    ports:
      - "3000:3000"
    environment:
      NEXT_PUBLIC_API_URL: "http://localhost:8100"
      NEXT_PUBLIC_RAG_URL: "http://localhost:8102"
      NEXT_PUBLIC_GRAPH_URL: "http://localhost:8108"
    depends_on:
      - notes-service
      - rag-service
      - ner-service
    networks:
      - rma-network
    restart: unless-stopped

  # ============================================
  # MCP N8N SERVICE
  # ============================================
  mcp-n8n:
    build:
      context: ./services/mcp-server
      dockerfile: Dockerfile
    container_name: rma-mcp-n8n
    ports:
      - "5678:5678"
    environment:
      N8N_HOST: "localhost"
      N8N_PORT: "5678"
      ENVIRONMENT: "production"
    volumes:
      - n8n-data:/home/node/.n8n
    networks:
      - rma-network
    restart: unless-stopped

# ============================================
# NETWORKS
# ============================================
networks:
  rma-network:
    driver: bridge

# ============================================
# VOLUMES
# ============================================
volumes:
  vllm-cache:
    driver: local
  ollama-vision-data:
    driver: local
  neo4j-data:
    driver: local
  postgres-data:
    driver: local
  redis-data:
    driver: local
  chromadb-data:
    driver: local
  n8n-data:
    driver: local
