# Architecture Comparison: Before vs After

## Before: Separate Ollama Instances

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RAG Demo Container              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama (GPU)                 â”‚  â”‚
â”‚  â”‚  - llama3.2 (~4GB GPU RAM)    â”‚  â”‚
â”‚  â”‚  - nomic-embed-text           â”‚  â”‚
â”‚  â”‚  Port: 11434                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  RAG App                      â”‚  â”‚
â”‚  â”‚  - Vector Store               â”‚  â”‚
â”‚  â”‚  Port: 8000                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Total: ~4GB GPU RAM


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     OCR Demo Container              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Ollama (GPU)                 â”‚  â”‚
â”‚  â”‚  - llama3.2 (~4GB GPU RAM)    â”‚  â”‚
â”‚  â”‚  - nomic-embed-text           â”‚  â”‚
â”‚  â”‚  Port: 11434 â† CONFLICT!      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  OCR App                      â”‚  â”‚
â”‚  â”‚  - Document Processing        â”‚  â”‚
â”‚  â”‚  Port: 5001                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Total: ~4GB GPU RAM

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Combined Total: ~8GB GPU RAM
Problems: Port conflicts, duplicate models
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

## After: Shared Ollama Service

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Shared Ollama Service Container   â”‚
                â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                â”‚  â”‚  Ollama (GPU)                 â”‚  â”‚
                â”‚  â”‚  - llama3.2 (~4GB GPU RAM)    â”‚  â”‚
                â”‚  â”‚  - nomic-embed-text           â”‚  â”‚
                â”‚  â”‚  Container: shared-ollama-    â”‚  â”‚
                â”‚  â”‚             service           â”‚  â”‚
                â”‚  â”‚  Port: 11434                  â”‚  â”‚
                â”‚  â”‚  Network: shared-llm-network  â”‚  â”‚
                â”‚  â”‚  Volume: shared_ollama_models â”‚  â”‚
                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RAG Demo Container      â”‚           â”‚  OCR Demo Container      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚           â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  RAG App           â”‚  â”‚           â”‚  â”‚  OCR App           â”‚  â”‚
â”‚  â”‚  - Vector Store    â”‚  â”‚           â”‚  â”‚  - Doc Processing  â”‚  â”‚
â”‚  â”‚  - Query Engine    â”‚  â”‚           â”‚  â”‚  - Text Extract    â”‚  â”‚
â”‚  â”‚  Port: 8000        â”‚  â”‚           â”‚  â”‚  Port: 5001        â”‚  â”‚
â”‚  â”‚  RAM: ~500MB       â”‚  â”‚           â”‚  â”‚  RAM: ~300MB       â”‚  â”‚
â”‚  â”‚  No GPU needed     â”‚  â”‚           â”‚  â”‚  No GPU needed     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  Container: rag-demo-app â”‚           â”‚  Container: ocr-demo-app â”‚
â”‚  Network: shared-llm-    â”‚           â”‚  Network: shared-llm-    â”‚
â”‚           network        â”‚           â”‚           network        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Combined Total: ~4GB GPU RAM + ~800MB System RAM
Benefits: 50% GPU savings, no conflicts, single model cache
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

## Request Flow

### RAG Demo Query Flow
```
User Browser (localhost:8000)
    â”‚
    â”‚ HTTP POST /query {"question": "What are Julian's hobbies?"}
    â–¼
RAG App Container (rag-demo-app)
    â”‚
    â”‚ 1. Embed question â†’ nomic-embed-text
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                      â”‚
    â”‚                                      â–¼
    â”‚                          Ollama Container
    â”‚                          (shared-ollama-service:11434)
    â”‚                                      â”‚
    â”‚                                      â”‚ Returns: embedding vector
    â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ 2. Search vector store
    â–¼
ChromaDB (local)
    â”‚
    â”‚ Returns: Top 4 relevant chunks about Julian
    â–¼
RAG App
    â”‚
    â”‚ 3. Generate answer with context
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                      â”‚
    â”‚                                      â–¼
    â”‚                          Ollama Container
    â”‚                          (llama3.2 model)
    â”‚                                      â”‚
    â”‚                                      â”‚ Returns: "Julian enjoys..."
    â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ 4. Return answer + sources
    â–¼
User Browser
    â”‚
    â””â”€ Display: Answer with source files
```

### OCR Demo Processing Flow
```
User Browser (localhost:5001)
    â”‚
    â”‚ HTTP POST /upload (PDF file)
    â–¼
OCR App Container (ocr-demo-app)
    â”‚
    â”‚ 1. Extract text from PDF
    â–¼
PDF Processing (local)
    â”‚
    â”‚ Returns: Raw text
    â–¼
OCR App
    â”‚
    â”‚ 2. Parse & structure text
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                      â”‚
    â”‚                                      â–¼
    â”‚                          Ollama Container
    â”‚                          (llama3.2 model)
    â”‚                                      â”‚
    â”‚                                      â”‚ Returns: Structured JSON
    â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚ 3. Save & return result
    â–¼
User Browser
    â”‚
    â””â”€ Display: Processed document
```

## Resource Usage Timeline

### Before (Separate Ollama)
```
Time â†’
GPU RAM  â”‚ â”Œâ”€â”€â”€â”€â”€RAG Ollama (4GB)â”€â”€â”€â”€â”€â”
8GB      â”‚ â”‚                          â”‚
         â”‚ â”‚                          â”‚
4GB      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ â”Œâ”€â”€â”€â”€â”€OCR Ollama (4GB)â”€â”€â”€â”€â”€â”
         â”‚ â”‚                          â”‚
0GB      â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Problem: 8GB total if both running
```

### After (Shared Ollama)
```
Time â†’
GPU RAM  â”‚ â”Œâ”€â”€â”€â”€â”€Shared Ollama (4GB)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
4GB      â”‚ â”‚ RAG req â†’ OCR req â†’ RAG req â†’   â”‚
         â”‚ â”‚ (sequential, queued internally)  â”‚
         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
0GB      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         Solution: 4GB total, requests queued
```

## Startup Sequence

### Manual Startup
```
Step 1: Start Ollama
$ docker-compose -f docker-compose.ollama.yml up -d
    â””â†’ Creates: shared-llm-network
    â””â†’ Starts: shared-ollama-service
    â””â†’ Pulls: llama3.2, nomic-embed-text
    â””â†’ Ready: ~30 seconds

Step 2: Start RAG Demo
$ cd OllamaRAGDemo && docker-compose up -d
    â””â†’ Connects to: shared-llm-network
    â””â†’ Starts: rag-demo-app
    â””â†’ Ingests: documents â†’ vector store
    â””â†’ Ready: ~60 seconds

Step 3: Start OCR Demo
$ cd ../OCRDemo && docker-compose up -d
    â””â†’ Connects to: shared-llm-network
    â””â†’ Starts: ocr-demo-app
    â””â†’ Ready: ~10 seconds

Total Time: ~2 minutes
```

### Automated Startup
```
$ ./start-all-demos.sh
    â””â†’ All steps automated
    â””â†’ Progress displayed
    â””â†’ Total: ~2 minutes
    â””â†’ Ready: All services up
```

## Network Topology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Docker Host (your machine)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    shared-llm-network (bridge)           â”‚  â”‚
â”‚  â”‚                                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  shared-ollama-service             â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  IP: 172.X.X.2                     â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  rag-demo-app                      â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  IP: 172.X.X.3                     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  Env: OLLAMA_BASE_URL=http://      â”‚ â”‚  â”‚
â”‚  â”‚  â”‚       shared-ollama-service:11434  â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â”‚                                          â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  ocr-demo-app                      â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  IP: 172.X.X.4                     â”‚ â”‚  â”‚
â”‚  â”‚  â”‚  Env: OLLAMA_BASE_URL=http://      â”‚ â”‚  â”‚
â”‚  â”‚  â”‚       shared-ollama-service:11434  â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                â”‚
â”‚  Port Mappings to Host:                        â”‚
â”‚  â€¢ 11434:11434 â†’ shared-ollama-service         â”‚
â”‚  â€¢ 8000:8000 â†’ rag-demo-app                    â”‚
â”‚  â€¢ 5001:5001 â†’ ocr-demo-app                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Benefits Summary

### Resource Efficiency
âœ… 50% GPU RAM savings (4GB vs 8GB)
âœ… 30% system RAM savings (2.8GB vs 4GB)
âœ… 50% disk space savings (4GB vs 8GB models)
âœ… Single model cache in memory

### Operational Benefits
âœ… No port conflicts
âœ… Faster app startup (model pre-loaded)
âœ… Consistent model versions
âœ… Easy to add more apps
âœ… Independent app lifecycle

### Development Benefits
âœ… Single command startup: `./start-all-demos.sh`
âœ… Single command shutdown: `./stop-all-demos.sh`
âœ… Easy to debug (centralized logs)
âœ… Simple to scale

### Cost Benefits
âœ… Smaller GPU requirements
âœ… Lower cloud costs (if deployed)
âœ… More efficient resource utilization
âœ… Can run more services on same hardware

Perfect for your demonstration tomorrow! ğŸš€
