version: '3.8'

# Docker Compose with LOCAL parsing (no cloud services)
# Privacy-first configuration for sensitive documents

services:
  ollama:
    image: ollama/ollama:latest
    container_name: rma-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    environment:
      - OLLAMA_HOST=0.0.0.0
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    devices:
      - /dev/nvidia0:/dev/nvidia0
      - /dev/nvidia-uvm:/dev/nvidia-uvm
      - /dev/nvidia-uvm-tools:/dev/nvidia-uvm-tools
      - /dev/nvidiactl:/dev/nvidiactl      
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  chromadb:
    image: chromadb/chroma:0.4.24  # Pin to 0.4.24 for compatibility with Python client
    container_name: rma-chromadb
    ports:
      - "8005:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - CHROMA_SERVER_CORS_ALLOW_ORIGINS=["*"]
      - CHROMA_SERVER_HOST=0.0.0.0
      - CHROMA_SERVER_HTTP_PORT=8000
    restart: unless-stopped

  notes-service:
    build:
      context: ./services/notes-service
      dockerfile: Dockerfile
    container_name: rma-notes-service
    ports:
      - "8100:8100"
    environment:
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped

  doc-processor:
    build:
      context: ./services/doc-processor
      dockerfile: Dockerfile.local
    container_name: rma-doc-processor
    ports:
      - "8101:8101"
    environment:
      # Enable local parsing (no cloud)
      - USE_LOCAL_PARSING=true
      - OLLAMA_URL=http://ollama:11434
      - VISION_MODEL=llava:7b
      - TEXT_MODEL=llama3.2
      # Optional: LlamaParse fallback (leave empty for local-only)
      - LLAMA_PARSE_API_KEY=
    depends_on:
      ollama:
        condition: service_healthy
    restart: unless-stopped

  rag-service:
    build:
      context: ./services/rag-service
      dockerfile: Dockerfile
    container_name: rma-rag-service
    ports:
      - "8102:8102"
    volumes:
      - ./manuals:/manuals
    environment:
      - OLLAMA_URL=http://ollama:11434
      - CHROMADB_HOST=chromadb
      - CHROMADB_PORT=8000
      - MANUALS_PATH=/manuals
    depends_on:
      ollama:
        condition: service_healthy
      chromadb:
        condition: service_started
    restart: unless-stopped

  client-rag-service:
    build:
      context: ./services/client-rag-service
      dockerfile: Dockerfile
    container_name: rma-client-rag-service
    ports:
      - "8104:8104"
    environment:
      - OLLAMA_URL=http://ollama:11434
      - CHROMADB_HOST=chromadb
      - CHROMADB_PORT=8000
    depends_on:
      ollama:
        condition: service_healthy
      chromadb:
        condition: service_started
    restart: unless-stopped

  upload-service:
    build:
      context: ./services/upload-service
      dockerfile: Dockerfile
    container_name: rma-upload-service
    ports:
      - "8103:8103"
    volumes:
      - upload_data:/data/uploads
    environment:
      - JWT_SECRET=${JWT_SECRET:-change-this-in-production}
      - UPLOAD_DIR=/data/uploads
      - DOC_PROCESSOR_URL=http://doc-processor:8101
      - CLIENT_RAG_URL=http://client-rag-service:8104
      - APP_BASE_URL=${APP_BASE_URL:-http://localhost:3000}
    depends_on:
      - doc-processor
      - client-rag-service
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - NEXT_PUBLIC_NOTES_SERVICE_URL=http://${SERVER_HOST:-localhost}:8100
        - NEXT_PUBLIC_DOC_PROCESSOR_URL=http://${SERVER_HOST:-localhost}:8101
        - NEXT_PUBLIC_RAG_SERVICE_URL=http://${SERVER_HOST:-localhost}:8102
        - NEXT_PUBLIC_UPLOAD_SERVICE_URL=http://${SERVER_HOST:-localhost}:8103
    container_name: rma-frontend
    ports:
      - "3000:3000"
    environment:
      # Use SERVER_HOST env var or default to localhost for local dev
      # For external access: export SERVER_HOST=192.168.5.70 (or your server IP/hostname)
      - NEXT_PUBLIC_NOTES_SERVICE_URL=http://${SERVER_HOST:-localhost}:8100
      - NEXT_PUBLIC_DOC_PROCESSOR_URL=http://${SERVER_HOST:-localhost}:8101
      - NEXT_PUBLIC_RAG_SERVICE_URL=http://${SERVER_HOST:-localhost}:8102
      - NEXT_PUBLIC_UPLOAD_SERVICE_URL=http://${SERVER_HOST:-localhost}:8103
    depends_on:
      - notes-service
      - doc-processor
      - rag-service
      - upload-service
    restart: unless-stopped

volumes:
  ollama_data:
  chroma_data:  # Shared ChromaDB storage for all collections (manuals + client docs)
  upload_data:

networks:
  default:
    name: rma-network

# PRIVACY NOTES:
# This configuration uses LOCAL document parsing only.
# No documents are sent to external cloud services.
# All processing happens on your infrastructure.
# GDPR compliant for sensitive financial documents.
