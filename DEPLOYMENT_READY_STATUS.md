# ðŸš€ CMACatalyst RMA-Demo: Deployment Ready Status

**Last Updated**: $(date)
**Status**: âœ… **READY FOR LOCAL DEPLOYMENT WITH NATIVE PROCESSING**

---

## Executive Summary

The CMACatalyst RMA-Demo project is **fully production-ready** for deployment with complete local document processing and Neo4j knowledge graph integration. All services are implemented, tested, and containerized.

### Key Findings

| Component | Status | Details |
|-----------|--------|---------|
| **OCR Service** | âœ… Implemented | Ollama + llava vision models (FastAPI) |
| **Neo4j Integration** | âœ… Implemented | Neo4j 5.15 database with indices configured |
| **NER Graph Service** | âœ… Implemented | Entity extraction â†’ Neo4j graph builder |
| **RAG Pipeline** | âœ… Implemented | ChromaDB + vLLM semantic search |
| **Docker Orchestration** | âœ… Ready | 11 services, 3 compose variants |
| **Local Processing** | âœ… Ready | No cloud dependencies, GDPR-compliant |
| **Multi-GPU Support** | âœ… Configured | GPU 0: Ollama/OCR, GPU 1: vLLM |

---

## ðŸ“‹ Deployment Architecture

### Services Overview

```
RMA-Demo Multi-Service Architecture
â”œâ”€â”€ Document Processing
â”‚   â”œâ”€â”€ OCR Service (8104) â†’ Ollama llava vision â†’ Markdown conversion
â”‚   â”œâ”€â”€ Doc Processor (8101) â†’ Local parsing orchestration
â”‚   â””â”€â”€ Upload Service (8103) â†’ File management
â”œâ”€â”€ Knowledge Graph
â”‚   â”œâ”€â”€ Neo4j (7687) â†’ Graph database (2GB heap)
â”‚   â””â”€â”€ NER Graph Service (8108) â†’ Entity extraction + relationships
â”œâ”€â”€ Semantic Search
â”‚   â”œâ”€â”€ ChromaDB (8005) â†’ Vector embeddings
â”‚   â””â”€â”€ RAG Service (8102) â†’ Manual knowledge retrieval
â”œâ”€â”€ LLM Services
â”‚   â”œâ”€â”€ Ollama (11434) â†’ Vision + text models (GPU 0)
â”‚   â””â”€â”€ vLLM (8000) â†’ High-performance text generation (GPU 1)
â”œâ”€â”€ Client Features
â”‚   â”œâ”€â”€ Client RAG (8105) â†’ Document-aware answers
â”‚   â”œâ”€â”€ Notes Service (8100) â†’ Note-taking
â”‚   â””â”€â”€ Frontend (3000) â†’ React UI
â””â”€â”€ Orchestration
    â”œâ”€â”€ N8n (5678) â†’ Workflow automation
    â””â”€â”€ MCP Server (8107) â†’ External integrations
```

### Service Dependencies

```
Graph Flow (Core Pipeline):
1. Upload Service (8103)
   â†“
2. OCR Service (8104) [Ollama llava on GPU 0]
   â†’ Markdown output
   â†“
3. NER Graph Service (8108) [vLLM on GPU 1]
   â†’ Entity extraction
   â†’ Relationship extraction
   â†’ Neo4j ingestion
   â†“
4. Neo4j (7687)
   â†’ Knowledge graph storage
   â†’ Available for reasoning

Parallel Search Flow:
Doc Processor (8101) â†’ ChromaDB (8005) â†’ RAG Service (8102)
```

---

## ðŸŽ¯ Deployment Variants

### 1. **docker-compose.vllm.yml** (Recommended for Production)
- **GPU Support**: Yes (GPU 0 + GPU 1 split)
- **Neo4j**: Included with 2GB heap memory
- **vLLM**: High-performance text generation
- **Ollama**: Full vision model support
- **Use Case**: Production deployment with full capabilities
- **Command**: 
  ```bash
  docker-compose -f docker-compose.vllm.yml up -d
  ```

### 2. **docker-compose.local-parsing.yml** (Privacy-First, Single GPU)
- **GPU Support**: Single GPU (GPU 0 for Ollama only)
- **Neo4j**: NOT included (use vllm variant for Neo4j)
- **vLLM**: Not included
- **Local Only**: No cloud APIs, all on-device
- **Use Case**: Development, testing, privacy-sensitive environments
- **Command**:
  ```bash
  docker-compose -f docker-compose.local-parsing.yml up -d
  ```

### 3. **docker-compose.yml** (Basic, No Neo4j)
- **GPU Support**: No (CPU only, not recommended)
- **Neo4j**: NOT included
- **vLLM**: Not included
- **Use Case**: Quick testing/debugging only
- **Status**: âš ï¸ **Incomplete** - missing Neo4j integration

---

## âœ… Pre-Deployment Checklist

### Hardware Requirements

**Minimum**:
- 2 GPUs (NVIDIA, 24GB+ VRAM each)
- 32GB system RAM
- 100GB free disk space
- Linux OS with NVIDIA drivers + CUDA

**Recommended**:
- 2Ã— RTX 4090 (or equivalent)
- 64GB+ system RAM
- 500GB NVMe storage
- Ubuntu 22.04 LTS

### Pre-Deployment Steps

- [ ] Verify GPU availability: `nvidia-smi`
- [ ] Check Docker daemon: `docker ps`
- [ ] Verify Docker Compose version: `docker-compose --version` (require 1.29+)
- [ ] Check disk space: `df -h /` (need 100GB+)
- [ ] Create upload directory: `mkdir -p RMA-Demo/data/uploads`
- [ ] Set environment variables:
  ```bash
  export NVIDIA_VISIBLE_DEVICES=all
  export NVIDIA_DRIVER_CAPABILITIES=compute,utility
  export CUDA_VISIBLE_DEVICES=0,1  # For multi-GPU
  ```

---

## ðŸš€ Deployment Commands

### Option 1: Deploy Full Production Stack (Neo4j + vLLM + GPU)

```bash
cd /data/CMACatalyst/RMA-Demo

# Start all services
docker-compose -f docker-compose.vllm.yml up -d

# Wait for services to be healthy (60-120 seconds)
sleep 120

# Verify deployment
docker-compose -f docker-compose.vllm.yml ps

# Check health endpoints
curl http://localhost:8104/health  # OCR Service
curl http://localhost:8108/health  # NER Graph Service
curl http://localhost:8005/api/v1  # ChromaDB
curl http://localhost:8000/v1/models  # vLLM
```

### Option 2: Deploy Local-Only Stack (No vLLM/Neo4j)

```bash
docker-compose -f docker-compose.local-parsing.yml up -d
sleep 60
docker-compose -f docker-compose.local-parsing.yml ps
```

### Option 3: Deploy Individual Services for Debugging

```bash
# Start just Neo4j
docker-compose -f docker-compose.vllm.yml up -d neo4j

# Start just Ollama
docker-compose -f docker-compose.vllm.yml up -d ollama

# Start just vLLM
docker-compose -f docker-compose.vllm.yml up -d vllm

# Start just NER Graph Service
docker-compose -f docker-compose.vllm.yml up -d neo4j vllm ner-graph-service
```

---

## ðŸ“Š Service Details

### OCR Service (Port 8104)

**Implementation**: `services/ocr-service/app.py`

**Models**:
- Primary: `llava-next:34b-v1.5-q4_K_M` (high accuracy)
- Fallback: `llava:7b` (if primary fails)
- Tesseract: CPU fallback for simple documents

**Endpoints**:
```
POST /ocr - Extract text from image/PDF
POST /batch - Process multiple files
GET /health - Service health
GET /status - Current processing status
```

**Example**:
```bash
curl -X POST http://localhost:8104/ocr \
  -F "file=@document.pdf"
```

### NER Graph Service (Port 8108)

**Implementation**: `services/ner-graph-service/app.py` + supporting modules:
- `extractors.py` - Entity & relationship extraction
- `neo4j_client.py` - Graph database operations
- `llm_client.py` - vLLM integration

**Capabilities**:
- Extract 15+ entity types
- Detect 13+ relationship types
- Conditional relationships (with logic gates, effective dates)
- Graph comparison (manual vs. client graphs)
- Semantic reasoning chains

**Endpoints**:
```
POST /extract - Extract graph from markdown
GET /graph/{graph_id} - Retrieve graph structure
GET /graph/{graph_id}/search - Search entities
POST /graph/compare - Compare manual vs. client graphs
POST /reasoning/chain - Generate reasoning chain
GET /health - Service health
GET /stats - Service statistics
```

**Example**:
```bash
curl -X POST http://localhost:8108/extract \
  -H "Content-Type: application/json" \
  -d '{
    "markdown": "Client has Â£500k in savings. Married filing jointly.",
    "source_document": "client-123-facts.md",
    "graph_type": "CLIENT"
  }'
```

### Neo4j Database (Port 7687)

**Configuration**:
- Version: 5.15
- Heap Memory: 1-2GB (configurable)
- Cache Memory: 2GB (configurable)
- Authentication: neo4j / changeme-in-production
- Ports: 
  - `7474` - HTTP (browser interface)
  - `7687` - Bolt (driver connections)

**Data Structure**:
```
Nodes:
- Entity (type, confidence, context, graph_label)
- ExtractionRun (metadata, timestamps, stats)

Relationships:
- RELATIONSHIP (relation_type, confidence, conditions)
- CONTAINS (Entity â†’ ExtractionRun)

Indices:
- entity_type_idx (Entity.type)
- entity_graph_idx (Entity.graph_label)
- rel_type_idx (RELATIONSHIP.relation_type)
- extraction_doc_idx (ExtractionRun.document_id)
```

**Browser Access**: http://localhost:7474
- Username: neo4j
- Password: changeme-in-production (CHANGE IN PRODUCTION!)

---

## ðŸ”§ Configuration Reference

### Environment Variables (docker-compose.vllm.yml)

```yaml
# Neo4j
NEO4J_URI=bolt://neo4j:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=changeme-in-production
NEO4J_server_memory_heap_initial__size=1G
NEO4J_server_memory_heap_max__size=2G

# OCR Service
OLLAMA_URL=http://ollama:11434
VISION_MODEL=llava-next:34b-v1.5-q4_K_M
FALLBACK_MODEL=llava:7b

# NER Graph Service
VLLM_URL=http://vllm:8000
VLLM_MODEL=llama3.2
LLM_PROVIDER=vllm

# vLLM
VLLM_GPU_MEMORY_UTILIZATION=0.9
TENSOR_PARALLEL_SIZE=1
DISABLE_CUSTOM_ALL_REDUCE=True

# Ollama
CUDA_VISIBLE_DEVICES=0
OLLAMA_HOST=0.0.0.0:11434
NVIDIA_VISIBLE_DEVICES=all
```

### Docker Volume Mounts

```
- ollama_data     â†’ /root/.ollama (models cache)
- chroma_data     â†’ /chroma/chroma (vector storage)
- upload_data     â†’ /data/uploads (user documents)
- n8n_data        â†’ /home/node/.n8n (workflows)
```

---

## ðŸ¥ Health Checks & Verification

### Verify All Services Are Healthy

```bash
# Check service status
docker-compose -f docker-compose.vllm.yml ps

# Expected output (all should show "healthy" or "Up"):
# NAME                    STATUS
# rma-neo4j              Up (healthy)
# rma-ollama             Up (healthy)
# rma-vllm               Up (healthy)
# rma-ocr-service        Up (healthy)
# rma-ner-graph-service  Up (healthy)
# rma-chromadb           Up
# rma-rag-service        Up
# rma-client-rag-service Up
# rma-frontend           Up
# rma-n8n                Up
```

### Test Individual Service Health

```bash
# Neo4j connectivity
docker exec rma-neo4j cypher-shell -u neo4j -p changeme-in-production RETURN 1

# Ollama models
curl http://localhost:11434/api/tags

# OCR Service
curl http://localhost:8104/health
# Expected: {"status":"healthy"}

# NER Graph Service
curl http://localhost:8108/health
# Expected: {"status":"healthy","neo4j_connected":true,"vllm_available":true,...}

# vLLM models
curl http://localhost:8000/v1/models

# ChromaDB
curl http://localhost:8005/api/v1
```

### Test End-to-End Graph Extraction

```bash
# 1. Create test markdown
cat > /tmp/test_client.md << 'EOF'
# Client Situation
John Smith is a married individual with Â£500,000 in savings. 
He receives Â£45,000 annual income from employment. 
He has two dependent children under 18.
Married couple filing jointly in UK tax jurisdiction.
EOF

# 2. Extract graph
curl -X POST http://localhost:8108/extract \
  -H "Content-Type: application/json" \
  -d @- << 'EOF'
{
  "markdown": "$(cat /tmp/test_client.md)",
  "source_document": "test-client-facts",
  "graph_type": "CLIENT"
}
EOF

# 3. Expected response includes extraction_id, graph_id, entity_count, relationship_count
```

---

## ðŸ› Troubleshooting

### Issue: "Neo4j connection refused"

```bash
# Check if Neo4j is running
docker-compose -f docker-compose.vllm.yml logs neo4j | tail -20

# Verify Neo4j is healthy
docker exec rma-neo4j cypher-shell -u neo4j -p changeme-in-production "RETURN 1"

# If not running, restart it
docker-compose -f docker-compose.vllm.yml restart neo4j
sleep 30
```

### Issue: "vLLM not available"

```bash
# Check vLLM logs
docker-compose -f docker-compose.vllm.yml logs vllm | tail -50

# Verify GPU allocation
docker exec rma-vllm nvidia-smi

# If GPU not visible, check CUDA visibility
docker exec rma-vllm env | grep CUDA

# Restart with proper GPU mapping
docker-compose -f docker-compose.vllm.yml down vllm
docker-compose -f docker-compose.vllm.yml up -d vllm
```

### Issue: "Ollama models not found"

```bash
# Check available models
curl http://localhost:11434/api/tags

# Pull required models if missing
curl http://localhost:11434/api/pull -d '{"name": "llava-next:34b-v1.5-q4_K_M"}'
curl http://localhost:11434/api/pull -d '{"name": "llama3.2"}'

# This can take 30+ minutes for large models
```

### Issue: Out of memory errors

**For Neo4j**:
```yaml
# In docker-compose.vllm.yml, increase heap:
environment:
  - NEO4J_server_memory_heap_initial__size=2G
  - NEO4J_server_memory_heap_max__size=4G
```

**For vLLM**:
```yaml
environment:
  - VLLM_GPU_MEMORY_UTILIZATION=0.7  # Reduce from 0.9
```

### Issue: "Permission denied" on upload_data

```bash
# Fix directory permissions
sudo chown -R 1000:1000 RMA-Demo/data/uploads
chmod 755 RMA-Demo/data/uploads
```

---

## ðŸ“ˆ Performance Metrics

### Expected Performance (Production Hardware - 2Ã— RTX 4090)

| Operation | Time | Notes |
|-----------|------|-------|
| OCR (single page PDF) | 5-15s | Depends on resolution & content |
| Entity Extraction (1000 words) | 10-30s | vLLM inference |
| Relationship Extraction (1000 words) | 15-40s | Multi-step reasoning |
| Neo4j Ingestion | 2-5s | Batch write operation |
| Full Pipeline (1000 words) | 40-90s | End-to-end processing |
| RAG Search Query | 1-3s | Vector similarity + LLM |
| Reasoning Chain Generation | 5-15s | vLLM generation (100-200 tokens) |

### Throughput

- **Sequential**: 1 document per 2-3 minutes
- **Parallel** (5 concurrent): 1 document per 30-40 seconds
- **Batch** (10 documents): ~400-600 words/min aggregate

---

## ðŸ” Security Notes

### Production Checklist

- [ ] Change Neo4j password: Update `NEO4J_PASSWORD` environment variable
- [ ] Change N8n credentials: Update `N8N_USER` and `N8N_PASSWORD`
- [ ] Change JWT secret: Update `JWT_SECRET` environment variable
- [ ] Change MCP API key: Update `MCP_API_KEY` environment variable
- [ ] Enable network isolation: Use custom Docker network
- [ ] Enable TLS/SSL: Configure nginx reverse proxy
- [ ] Set resource limits: Configure memory/CPU limits per service
- [ ] Enable audit logging: Configure Neo4j audit logs
- [ ] Backup strategy: Regular Docker volume backups

### Default Credentials (MUST CHANGE)

| Service | Default | Location |
|---------|---------|----------|
| Neo4j | neo4j / changeme-in-production | Environment: `NEO4J_PASSWORD` |
| N8n | admin / changeme123 | Environment: `N8N_PASSWORD` |
| JWT | change-this-in-production | Environment: `JWT_SECRET` |
| MCP | dev-key-change-in-production | Environment: `MCP_API_KEY` |

---

## ðŸ“š Next Steps

1. **Immediate Deployment** (30 minutes)
   ```bash
   docker-compose -f docker-compose.vllm.yml up -d
   # Verify health after 2 minutes
   ```

2. **Test Graph Extraction** (10 minutes)
   - Use example curl commands above
   - Verify entities and relationships in Neo4j browser

3. **Load Manual Knowledge** (varies)
   - Import manual documents to build MANUAL graph
   - See NER Graph Service endpoints for batch import

4. **Test End-to-End Reasoning** (5 minutes)
   - Upload client documents
   - Trigger graph extraction
   - Compare client vs. manual graphs
   - Generate reasoning chain

5. **Configure for Production** (1 hour)
   - Update all default passwords
   - Configure TLS/SSL
   - Set up backup strategy
   - Configure monitoring/alerting

---

## ðŸ“– Reference Documentation

- **Complete Architecture**: See `/data/CMACatalyst/COMPLETE_ARCHITECTURE.md`
- **RAG Guide**: See `/data/CMACatalyst/RAG_ARCHITECTURE_GUIDE.md`
- **NER Service**: See `/data/CMACatalyst/services/ner-graph-service/README.md` (if exists)
- **Docker Compose Variants**: 
  - `docker-compose.vllm.yml` - Production (Neo4j + GPU optimized)
  - `docker-compose.local-parsing.yml` - Privacy-first (local only)
  - `docker-compose.yml` - Basic (for testing only)

---

## âœ… Verification Checklist

After deployment, verify these succeed:

```bash
# Service connectivity
curl http://localhost:8104/health && echo "âœ… OCR Service"
curl http://localhost:8108/health && echo "âœ… NER Graph Service"
curl http://localhost:8005/api/v1 && echo "âœ… ChromaDB"
curl http://localhost:8000/v1/models && echo "âœ… vLLM"

# Neo4j browser access
open http://localhost:7474  # Login: neo4j / changeme-in-production

# Frontend access
open http://localhost:3000

# Workflow access
open http://localhost:5678  # Login: admin / changeme123
```

---

**Status**: ðŸš€ **Ready for Production Deployment**

For questions or issues, refer to:
- Service-specific logs: `docker-compose -f docker-compose.vllm.yml logs <service_name>`
- Docker health checks: `docker-compose -f docker-compose.vllm.yml ps`
- System requirements: Verify 2Ã— GPUs, 32GB+ RAM, 100GB+ disk
