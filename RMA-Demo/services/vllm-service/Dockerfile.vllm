# vLLM-based LLM inference server
# Provides OpenAI-compatible API for language model inference
# Optimized for throughput and latency compared to Ollama

FROM vllm/vllm-openai:v0.3.0

# Install additional dependencies
RUN apt-get update && apt-get install -y \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Create non-root user for security
RUN useradd -m -u 1000 vllm && chown -R vllm:vllm /home/vllm

USER vllm
WORKDIR /home/vllm

# Default model - will be overridden by environment variable
ENV MODEL_NAME=meta-llama/Llama-2-7b-hf
ENV GPU_MEMORY_UTILIZATION=0.9
ENV MAX_MODEL_LEN=4096
ENV TENSOR_PARALLEL_SIZE=1

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=180s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Command to run vLLM server with OpenAI-compatible API
CMD python -m vllm.entrypoints.openai.api_server \
    --model ${MODEL_NAME} \
    --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION} \
    --max-model-len ${MAX_MODEL_LEN} \
    --tensor-parallel-size ${TENSOR_PARALLEL_SIZE} \
    --host 0.0.0.0 \
    --port 8000 \
    --served-model-name llm
